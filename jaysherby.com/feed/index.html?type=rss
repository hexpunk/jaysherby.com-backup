<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Jay's blog</title>
    <link>https://jaysherby.com/</link>
    <description>Hi! I'm Jay Sherby!

I'm an experienced software developer in Chicago.
Hiring? Check my r√©sum√© to see if we're a match....</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <item>
      <title>Sequelize And The Disappearing ID Column</title>
      <link>https://jaysherby.com/sequelize-and-the-disappearing-id-column/</link>
      <description>&lt;p&gt;I recently fixed a bug in some code that uses Sequelize. Let me set the scene and let's see if you can figure out what happened.Here's some example models that are sufficient to illustrate the situation I faced. I'll use a typical example domain. There is a table for books, a table for authors, and a join table to associate them since books can have multiple authors at once.&lt;code&gt;ts// Book.model.tsclass Book extends Model { static BookAuthors: HasMany&amp;lt;Book, Author&amp;gt;;}Book.init({ name: DataTypes.TEXT,}, { sequelize, modelName: 'Book',});Book.BookAuthors = Book.hasMany(BookAuthor);``````ts// Author.model.tsclass Author extends Model {}Author.init({ name: DataTypes.TEXT,}, { sequelize, modelName: 'Author',});``````ts// BookAuthor.model.tsclass BookAuthor extends Model { static Book: BelongsTo&amp;lt;BookAuthor, Book&amp;gt;; static Author: BelongsTo&amp;lt;BookAuthor, Author&amp;gt;;}BookAuthor.init({}, { sequelize, modelName: 'BookAuthor',});BookAuthor.Book = BookAuthor.belongsTo(Book);BookAuthor.Author = BookAuthor.belongsTo(Author);&lt;/code&gt;You may have already noticed something a little non-idiomatic. The Book model is using &lt;code&gt;hasMany&lt;/code&gt; instead of &lt;code&gt;belongsToMany&lt;/code&gt;. That's how this was set up in the actual code I was working in.Let's do a simple query to see what the data structure looks like.&lt;code&gt;tsconsole.log( JSON.stringify( ( await Book.findOne({ where: { id: 1 }, include: [{ model: BookAuthor, include: [{ model: Author }] }], }) )?.toJSON(), null, 2 ));``````json{ "id": 1, "name": "The Hobbit", "BookAuthors": [ { "id": 1, "BookId": 1, "AuthorId": 1, "Author": { "id": 1, "name": "J. R. R. Tolkien" } } ]}&lt;/code&gt;Fantastic.  No surprises there.Fast forward a couple of months and a bug was reported. An error was occurring because of a missing ID field in the results of that same query.Let's run the query again and see what we get.&lt;code&gt;diff--- before.json 2023-12-23 20:50:56.638119488 +0000+++ after.json 2023-12-23 20:50:59.674119628 +0000@@ -1,15 +1,14 @@ { "id": 1, "name": "The Hobbit", "BookAuthors": [ {- "id": 1, "BookId": 1, "AuthorId": 1, "Author": { "id": 1, "name": "J. R. R. Tolkien" } } ] }&lt;/code&gt;Sure enough, the &lt;code&gt;id&lt;/code&gt; of the join table is missing. The &lt;code&gt;id&lt;/code&gt; column of the table still existed. This was an active project, so many files had been changed between the previous release, which I could confirm still returned the &lt;code&gt;id&lt;/code&gt; of the join table, and the current version of the code. But the &lt;code&gt;BookAuthor.model.ts&lt;/code&gt; file &lt;em&gt;had not changed&lt;/em&gt;.I could work around the issue by adding &lt;code&gt;attributes: { include: ["id"] }&lt;/code&gt; like so:&lt;code&gt;tsconsole.log( JSON.stringify( ( await Book.findOne({ where: { id: 1 }, include: [ { model: BookAuthor, attributes: { include: ["id"] }, include: [{ model: Author }], }, ], }) )?.toJSON(), null, 2 ));&lt;/code&gt;But that didn't explain &lt;em&gt;why&lt;/em&gt; the &lt;code&gt;id&lt;/code&gt; was suddenly missing. It took a &lt;a href="https://git-scm.com/docs/git-bisect"&gt;git bisect&lt;/a&gt; script for me to figure it out. Git's bisect command is a very powerful tool and I recommend you add it to your tool belt if you haven't already.If you want to try to figure it out for yourself, stop here.---After running git bisect, the culprit was a seemingly innocent change to the &lt;code&gt;Author.model.ts&lt;/code&gt; file. A developer had added &lt;code&gt;belongsToMany&lt;/code&gt; to the Author model to make it easier to traverse the relationship between Author and Book.&lt;code&gt;ts// Author.model.tsclass Author extends Model { static Books: BelongsToMany&amp;lt;Author, Book&amp;gt;;}Author.init( { name: DataTypes.TEXT, }, { sequelize, modelName: "Author", });Author.Books = Author.belongsToMany(Book, { through: BookAuthor });&lt;/code&gt;Using the &lt;code&gt;belongsToMany&lt;/code&gt; function to mark the BookAuthor model definitively as a join table in the eyes of Sequelize causes Sequelize to hide the &lt;code&gt;id&lt;/code&gt; column. Sequelize uses the foreign keys of join tables as a complex unique primary key.When you look at it from the point of view of Sequelize's happy path, this makes complete sense. This is how Sequelize thinks join tables should be defined. When you look at it through a historical lens, it's surprising behavior that a change to some other model should cause a column to disappear from the results of a query.&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/sequelize-and-the-disappearing-id-column/</guid>
      <pubDate>Sat, 23 Dec 2023 21:14:31 +0000</pubDate>
    </item>
    <item>
      <title>Ode To A Desoldering Pump</title>
      <link>https://jaysherby.com/ode-to-a-desoldering-pump/</link>
      <description>&lt;p&gt;Sometimes I solder electronics. I take no joy in it. I do it as a means to an end. I consider myself an experienced beginner and hobbyist. &lt;em&gt;I am not an expert._The most personally frustrating part of soldering for me is correcting a mistake. I've soldered the wrong resistor into a spot on a really cramped board. I've already snipped the leads. Now what?Previously, I'd curse myself a few times, break out the desoldering braid, and pray. Desoldering braid, in my experience, is _terrible&lt;/em&gt;. I can count on one hand the number of occasions it worked without issue.&lt;img alt="Desoldering Pump" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/jaysherby-1696351068-0.jpg"/&gt;During my most recent solder session, I got the chance to finally break out my &lt;strong&gt;Hakko FR301-03/P Portable Desoldering Tool with Precise Temperature Control&lt;/strong&gt;. I'm going to warn you, it's eye-wateringly expensive, but it's worth every penny! I corrected my mistake in less than a minute.I don't blame you if you want to look around for cheaper alternatives. But I will never go back to braid or those cheap, spring-loaded solder suckers that never really work again. A heated tip with a vacuum pump is just too convenient.While the desoldering pump is the biggest game-changing tool I've added to my hobby electronics arsenal, I want to share some other recommendations that have vastly improved my experience.Most people know about "helping hands" tools that hold boards and components for you while you solder. The most common variations are a heavy chunk of metal with a bunch of ball joints with wing nuts to loosen and tighten the joint. They typically have two alligator clip "hands" and a magnifying glass. And they're &lt;em&gt;garbage&lt;/em&gt;.&lt;img alt="Omnifixo Helping Hands" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/jaysherby-1696351083-0.jpg"/&gt;Check out &lt;strong&gt;&lt;a href="https://omnifixo.com/"&gt;Omnifixo&lt;/a&gt;&lt;/strong&gt; instead. It's designed by a guy from Sweden, and it's amazing. Go check out the website to see its genius design and all of the clever little features. I can attest that it exceeds all expectations I had for a helping hands tool. If you're used to paying under $10 for the cheap versions of this tool, you might experience a bit of sticker shock. But this is another tool that's well worth the price.Finally, what the hell is &lt;em&gt;that&lt;/em&gt; component? I can't read the label on it. It's got a number printed on it. I searched it on the web and figured it out, but that took like 10 minutes. Now that I know what it is, how can I be sure it's even working?&lt;img alt="T7 Multi-function Component Tester" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/jaysherby-1696351116-0.jpg"/&gt;There's a tool for that. Search your favorite marketplace that drop-ships cheap knock-offs from China (i.e. Amazon, eBay, AliExpress, etc.) for "&lt;strong&gt;LCR-T7&lt;/strong&gt;" or "&lt;strong&gt;T7 Tester&lt;/strong&gt;". You should be able to find a small device with a screen, a teal zero insertion force socket, and a single yellow button for around $20.This thing is amazing! Plug in almost any passive component and it'll tell you what it is. It'll tell you the resistance of resistors, the capacitance of capacitors, and the direction of diodes. It'll identify the pins of transistors. And it does it all in mere seconds.&lt;em&gt;In case it wasn't obvious, I'm not making any commissions, kickbacks, or money of any kind by recommending these tools. The only one I linked to was the Omniflexo because you can only buy it from its website right now. There are no affiliate links or anything like that.&lt;/em&gt;&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/ode-to-a-desoldering-pump/</guid>
      <pubDate>Tue, 03 Oct 2023 16:19:55 +0000</pubDate>
    </item>
    <item>
      <title>Vintage Computer Festival Midwest 18</title>
      <link>https://jaysherby.com/vintage-computer-festival-midwest-18/</link>
      <description>&lt;p&gt;I visited &lt;a href="https://www.vcfmw.org/"&gt;Vintage Computer Festival Midwest&lt;/a&gt; today. This is my second year attending.# Last yearWhen I visited last year, the theme was Commodore 64 as it was the 40th anniversary of the popular micro's release. It was thrilling and a little overwhelming. I spent a fair amount of money on a C64 with all the modern bells and whistles. It needs a little love to strengthen the video signal, but it's in great condition otherwise.I'm embarrassed to say I haven't found the time to do the necessary repair, so the machine has been sitting atop my printer since I brought it home.# This yearI was a bit more disciplined with my spending this year and managed to only walk out with a commemorative t-shirt. That's not to say I didn't have a ton of fun though. Let me share some of my favorite experiences from this year's festival.## Flipper Slipper on a Spectravideo SV-328I'd never heard of &lt;a href="https://en.wikipedia.org/wiki/Spectravideo"&gt;Spectravideo&lt;/a&gt;, nor Flipper Slipper before today. It was found, appropriately, at the "Obscure Computers" exhibit. Flipper Slipper is a Breakout-like game with an aquatic motif. Although the theme and elements were bizarre, the game controlled well and the twist of adding half circle paddles that can cause interesting ball physics was satisfying.Flipper Slipper was also released for the ColecoVision. Although I can't attest that the ColecoVision version has the same fluent controls, it's probably an easier platform to emulate if you want to give the game a try.## The Emergency Alert SystemThe "Behind The Screens" table, returning from last year, caters to a niche but interesting subject. Featured were the hardware systems that generate automated cable channels like The Weather Channel and the Prevue Guide Channel, known as the TV Guide Channel on the cable package my parents had while I was growing up.The part of the exhibit that I had the most fun with was the Emergency Alert System hardware. A camera was pointed at the attendees. It fed into a vintage character generator you could use to overlay text. That was then output on a CRT TV. It's a clever gimmick for people who want to write a custom message on a selfie of themselves on TV.But within the loop was the EAS hardware. Next to the EAS box was a red telephone and an instruction card. By dialing a particular number, you were would choose a number corresponding to the type of emergency message you'd like to send, and record a voice message to be played during it. I chose "fire warning" and mumbled something about this being a test, just in case. Sure enough, those loud, familiar tone patterns played and an appropriate warning message appeared on the TV. I couldn't hear my message, but I'm sure it played and I just hadn't spoken loudly enough when I recorded it.It was a thrilling experience to trigger an emergency alert, even if it was just on a single television. In retrospect, I wish I had chosen a different type of message. A storm or tornado warning would be the most relatable. I did spot a choice in the list for a nuclear power plant warning. That sounds dramatic. I didn't get a chance to read the list of types completely, so maybe I missed some interesting options.## OS/2Although I didn't linger at the OS/2 exhibit, I was glad to see it return from last year. The first computer my family owned ran OS/2, so it's a real blast of nostalgia for me. I've independently collected a few different OS/2 installation media and other memorabilia including a beautiful enamel pin. Any acknowledgement of OS/2 is enough to put a smile on my face.## A step switch demonstrationYou may be surprised to find there's a significant number of telephony wonks at VCF Midwest. Apparently it's tradition to wire up a PBX private phone network that anyone can connect to if they bring a compatible device. Some machines are connected to host BBSes. Some connect automated phone systems, like the phone that was connected to the EAS I talked about earlier. Some people set up little Easter egg systems to encourage attendees to use wardialers to find them. It's a fascinating vintage computing subculture.The exhibit that hosts the PBX hardware to power the phone system is "Shadytel MidWest Telephone CO", also returning from last year. The PBX system they employ is a vintage system from the 1970s, but it's already completely digital. However, the exhibit includes a two-phone demonstration of an older electro-mechanical step switch telephone exchange system. Here's &lt;a href="https://www.youtube.com/watch?v=bvPH-tsD9ZM"&gt;a video&lt;/a&gt; with a very similar demonstration. It's impressively large with satisfying little movements and clicks. It feels very logical and elegant, given the technology available at the time of its creation. Chef's kiss, no notes.## Stuff for sale- I got to see the &lt;a href="https://www.core64.io/"&gt;Core64&lt;/a&gt; in person. It's very cool. The ferrite beads are even smaller than you think they are.- Altair 8800s! They're not cheap, so this is for the Serious Collector‚Ñ¢. They were plentiful this year. But if you want in one the fun without the hefty price tag, there were also much cheaper clones built from new designs.- More Commodore 64 SX units in one place than I've ever seen! And that includes last year's C64 celebration. They were surprisingly affordable, and in very good condition. I'm used to seeing them for eye-watering prices in rough condition on eBay.# Next yearI think VCF Midwest has outgrown its venue. It was a record attendance for the second year in a row. Things were awfully cramped.I had an idea for more attendee engagement that I'd love if someone stole for next year. I was following the #vcfmw hashtag on Mastodon to see other people's pictures of cool stuff. Why not encourage attendees to use the hashtag with a little signage and hook up a vintage computer to a large monitor or a projector and display people's messages?&lt;a href="https://github.com/SuperIlu/DOStodon"&gt;DOStodon&lt;/a&gt; on a Pentium would be a good choice since it has some image support. There's also a &lt;a href="https://github.com/FujiNetWIFI/fujinet-apps/tree/master/mastodon"&gt;FujiNet Mastodon app&lt;/a&gt; that works on the big 3 micros: Apple ][, C64, and Atari 8-bits.I hope to see you there next year!&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/vintage-computer-festival-midwest-18/</guid>
      <pubDate>Sun, 10 Sep 2023 01:42:53 +0000</pubDate>
    </item>
    <item>
      <title>Computer Science Primer For Writers And Authors</title>
      <link>https://jaysherby.com/computer-science-primer-for-writers-and-authors/</link>
      <description>&lt;p&gt;It's a very tumultuous time for creative humans at the moment. Writers, musicians, and artists are being told they're at risk of being replaced by AI. The irony is not lost on me that rather than trying to automate away the drudgery of life, AI tech bros seem focused on automating the very things that are uniquely human, the things most of us would prefer to spend our time on. Those who seek to commercialize these AI tools must not put much value on art if they think it can be automated and mass-produced. It's not just an insult to those who produce art, but to those who consume it.I read &lt;a href="https://www.techdirt.com/2023/08/08/the-fear-of-ai-just-killed-a-very-useful-tool/"&gt;an article from Techdirt&lt;/a&gt; today that concerned me, however. A person named Benji Smith created a website called Prosecraft that analyzed commercial books and provided interesting statistics. A number of authors whose books' statistics appeared on the site were very unhappy about it. The resulting backlash led to Prosecraft being taken down.The backlash, however, seems a bit misguided. If you're a computer scientist or a software developer, the reason it's misguided is probably obvious. If you're a layman, I completely understand your confusion. The Techdirt article doesn't really explain why lumping Prosecraft in with the AI that people are rightly concerned about is unfair.In this post, I aim to break down the terminology so that even if you don't know a nibble from a byte, you can have an informed opinion about artificial intelligence.## What is Artificial Intelligence?This question is at least as difficult as that classic philosophical quandary, what is art? The problem with defining artificial intelligence is rooted in the problem of defining intelligence. The answer is murky, at best.If you hear the term "artificial general intelligence", it generally refers to a machine or computer program exhibiting human-like intelligence that can be applied across subjects and disciplines. Artificial general intelligence, or AGI, does not exist at this point in time. Whether or not it's even possible is still debated.Besides being hard to define, artificial intelligence has historically been a moving goal post. Someone will develop an algorithm (fancy talk for a set of instructions to solve a given problem) that does something previously thought to be difficult or near impossible. It's lauded as "artificial intelligence"! That is, until the solution is studied and becomes well-understood. The field advances and that algorithm is no longer considered to be AI anymore.This may be a cynical view, but I believe that AI, in practice, is any cutting edge technology that provides solutions that appear human-like, to a problem that was until recently thought to be computationally difficult. As soon as it's no longer considered cutting edge, it's no longer considered AI.Let's move away from AI as a term as it's all but meaningless.## What is currently meant by "AI"?When you see "AI" being tossed around in the press, what's usually being discussed is the field of machine learning or ML. There are many different techniques being used right now, but you need to understand that this isn't magic. It's just statistical analysis.No matter what techniques are being used, it all basically boils down to creating a large and complicated mathematical expression to accomplish a task. The way this is done is through a process called "training". Training takes sets of data, referred to as training data, and uses it to adjust constants within the mathematical expression until the accuracy of the expression improves.Let's use a simple real-world example. Let's say I want an expression that will tell me whether or not there's a cat in a given image. My training data would be a large set of images comprised of two smaller sets, one with cats and one without. I've also labeled each set. Training the model consists of turning each image into a sequence of numbers, running that sequence through the expression, and seeing what number comes out of the expression. The number that comes out of the expression can be thought of as a probability that there's a cat in the image. But we already know there's a cat in the image because we labeled it beforehand. If the model produces a result of, say, 0.62 or 62% confidence, the training process will begin tweaking constants in the expression to get that number closer to 1.Only once training is completed can the model be used. In the example above, once my cat picture model has been trained on all of my example pictures, I can start using it to classify images that haven't been seen before. Machine learning models are not like humans. When I'm using it to classify pictures of cats it's never seen before, it's not "learning" anything new. The model no longer changes.[^1]That doesn't mean the model is set in stone. If I put together some new pictures of cats and, well, not cats, I can continue training where I left off. I don't necessarily need to start from scratch training a new model. But it wouldn't make sense to continue training the cat classifier model on unknown pictures. If I give it a picture and it thinks there's an 80% likelihood there's a cat in the picture, if I were to then train it on its own results, that would be a recipe for disaster. The meaning behind the training would lose cohesion. It might turn into a classifier for furry things, or a classifier for contrasting objects, or just junk that doesn't really classify anything at all.Generally speaking, the more training data we have to train the model, the more accurate the model will be.Although breakthroughs are happening all the time in the field of machine learning as new techniques emerge, no breakthrough can change the fact that more training data corresponds to better results. There's an insatiable appetite for training data right now. That's a primary reason why services like Reddit and Facebook are getting protective over their data and who can access it. And why companies like Zoom, who technically have access to tons of valuable data that can be used for ML training, are trying to pivot to cash in on that data and the access to it.It's also why individual creators are concerned about the implications of fair use laws. Just because I want to share art with fans via the web, why should companies be able to use it for free to train computer programs to undercut my profits while making art in my personal style? It's an absolutely valid question.Taken to a logical extreme, machine learning may be able to be used as a form of intellectual property laundering. &lt;strong&gt;Obligatory reminder that I am not a lawyer.&lt;/strong&gt; Machine learning sometimes exhibits a problem called "overfitting" where a model will spit out training data verbatim. This is generally considered an undesirable flaw, but it happens quite often. If I train a model on a copyrighted work and it spits out a facsimile of it, is that covered under fair use? I honestly don't know.A similar legal issue was raised during the early days of online music piracy. If Sony sells me a license to the song "Toxic" by Britney Spears, otherwise known as buying music digitally, what are they actually licensing to me? Is it the literal numbers that represent the MP3 file in storage? If so, what if I re-encode the file using different settings or for a different file format like WAV? The numbers that represent the file itself may not have any substantial overlap with the numbers of the MP3 they licensed to me. But if I play it back, my human ears can't tell the difference.I can't cite a court case, but that legal theory was shot down, of course. I'd bet it's only a matter of time before the legal meaning of fair use is changed to exclude machine learning training data. I'm still not a lawyer, though.## What did Prosecraft do?Now that you have a better understanding of the technology that has everyone concerned, let's look at Prosecraft, the website that was shut down due to artificial intelligence concerns.For a given book, Prosecraft would tell you how many words were in it. From there, it would give percentages about things like how many of the book's words were adverbs. Of the adverbs, how many ended in -ly or not.So far, this is purely in the realm known as natural language processing, or NLP. NLP is a wide field concerned mostly with linguistic statistical analysis aided by computer science. What we've seen Prosecraft do so far hasn't touched anything related to machine learning.If I wanted to replicate the abilities of Prosecraft we've discussed so far, I could do that by making a list of known adverbs from a dictionary. Then my theoretical computer program would process a book's text word by word. It would only have to keep track of how many words it processed, how many of them appear in my list of adverbs, and how many of the adverbs it finds end in -ly. There's no training, no learning, and no retention of the book text after analysis has completed.Prosecraft also offered analysis around a book's vividness versus passiveness. It provided percentages for each as well as showing out-of-context excerpts from the book of what it termed the most vivid page and the most passive page from the book, including color-coding words it considered vivid or passive within those excerpts.The analysis of vividness or passiveness gets a little murky because those can be considered subjective measures. My understanding of how this worked was via sentiment analysis.## What is sentiment analysis?Sentiment analysis is a subfield within natural language processing that seeks to automatically identify emotional meaning behind words. The sentence "Jeff Bezos was born in 1964" is purely factual and doesn't really have any sentiment behind it. The sentence "Jeff Bezos is disgusting and I hate him" has a very strong sentiment behind it.But language is tricky. If I said "Jeff Bezos is not a great guy and I don't love him" you understand this is still a negative sentiment. But a simple computer algorithm might just see "Jeff Bezos", "great guy", and "love him" and come to the wrong conclusion. It's this fuzziness that has led people to employ machine learning techniques for sentiment analysis.Prosecraft used sentiment analysis. I don't know what tools or algorithms were used and I don't know how they were used. But my best guess is that they were used to classify words and phrases as "vivid" or "passive voice".It's important to note that the books analyzed on Prosecraft were not used to &lt;em&gt;train&lt;/em&gt; any algorithms to detect these characteristics.## Wrapping upI hope I've made it clear that I understand the outrage and panic about AI. Don't believe the hype about artificial general intelligence. A more real threat of machine learning is its potential use to devalue art and the people who make art.However, I agree that Prosecraft was unfairly targeted. What Prosecraft did was provide statistical tools that could help writers and readers. Criticizing Prosecraft would be like criticizing spell check and grammar check tools, once considered AI in their own right!Fred Rogers said, "I went into television because I hated it so." I read that recently and I was floored! I never thought I'd hear Mr. Rogers say he hated anything. But he had the right attitude. When you're strongly repelled by something, it's important to learn as much as you can about it. I hope this post has helped someone learn a little more about something they despise. Just like television, machine learning is a tool. It has the potential to empower us. But it has a lot of potential for misuse as well. I want to make sure we know what we're railing against, because there's a real threat of collateral damage from modern day luddites.‚Äî[^1]: I think this is an important point about things like ChatGPT as well. We're often told to use tools like that cautiously and skeptically. We should be skeptical of what ChatGPT tells us because it might not be telling the truth. (Not lying per se, as that implies intent.) But we should be cautious about what we tell ChatGPT, not because it might learn our personal details and tell them to someone else, but because we're throwing data onto someone else's computer. ChatGPT isn't the risk. Hackers, humans, are the risk.&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/computer-science-primer-for-writers-and-authors/</guid>
      <pubDate>Thu, 10 Aug 2023 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Am I A "Real Developer" Yet?</title>
      <link>https://jaysherby.com/am-i-a-real-developer-yet/</link>
      <description>&lt;p&gt;I've fallen into the "real programmer" trap. Despite being gainfully employed as a software developer for all of the past 13 years, I'm finally feeling that impostor syndrome everyone keeps talking about.I started learning the Rust programming language maybe six years ago or so. I had mostly overcome that steep learning curve. I was starting to actually write non-trivial programs. I chose to use Rust to do my coding interview for a job I landed in early 2020. I was getting comfortable with it.Things changed, though. I went to a meetup of Rust users and found them to be just intolerable as a community. A new major version of Rust landed and my old code needed a some changes to run again, and a lot more changes to be considered idiomatic. The new async features landed. It felt like I had to learn a new language all over again.Maybe this would be acceptable if Rust was my daily language. Coding in Rust was a hobby. Depending on what is happening in my life, I may go weeks or even months between opportunities to return to my hobby projects. It wasn't a tenable choice if I wanted to &lt;em&gt;get things done&lt;/em&gt;.I resolved to learn C. Changes in C are infrequent. The language changes at a glacial pace. But it retains some of the features I found desirable in Rust. It runs very close to the metal compared to other languages, so it tends to be fast. Deployment of artifacts is as simple as sharing a single binary, assuming your dependencies are statically linked.The truth is, I'm struggling. And it's not the language itself. C as a language is pretty reasonable. Sure, there are some rough edges. Pointers aren't hard, but you can get tangled up in them pretty fast. Implicit integer conversion can be a real headache. Overall, though, it's pretty straightforward.What hamstrings C is not the language itself but rather its tooling, or lack thereof. I'm used to relying on a variety of tools when I develop software: formatters, linters, type checkers, unit test runners, package managers, task runners, etc. C's tooling story is spartan, and it really hinders my progress with the language. My experience developing in C is death by a thousand cuts.Let's start with compilation. Compiling a single C source file into an executable that only relies on the standard library can be done with a single command. As a program grows, I'm naturally going to want to separate my code into units in the form of files. There are tons of tools out there to help with this, but the most ubiquitous is Make.I struggle with Make. I've read multiple books, &lt;em&gt;whole-ass books&lt;/em&gt;, about Make and I still can't fully understand it. At its most basic, it's a language-agnostic tool for taking files as input and doing some process to them which then creates new files. Make is brilliant at this use case. It tracks timestamps on files to do the least amount of work necessary when you run your Makefile. I think my main problem with Make is due to how agnostic it is. I'm used to build systems where I only need to point the tool at my entry point and it can figure out the rest. Make can't do that because it doesn't know anything about C. That puts the responsibility on me to code my dependency graph into my Makefile. That's a lot of work! I'm not used to that amount of work to set up a build process. It's very frustrating when I decide to factor certain bits of code out into their own units because I have to update my Makefile as well. I haven't found any way to make this trivial yet. Ideally, I want my build tool to understand C source code enough that I can point it at my entry points and it can build the dependency graph for me.---C predates the concept of unit tests. That's kind of an insane statement. It explains why there are no default unit test facilities that ship with C's standard library. I'd expect unit tests to be a solved problem for any decently popular language by now. I can't seem to find any unit test libraries or frameworks for C that don't have a huge red flag. There's plenty of "single header file" projects that have a single developer who has thrown them onto Github ten years ago and then abandoned. There are ones that require &lt;em&gt;so much boilerplate&lt;/em&gt;. There are ones that have more dependencies than the program I'm writing. And there's Unity, which requires Ruby to run.I tried writing my own simple unit test setup. How hard can it be if legions of programmers have uploaded theirs on Github and abandoned them? The biggest source of boilerplate in writing unit tests in C seems to be having to add each unit test function to your runner. I made the function definition part of creating a test into a macro that adds the function to a list automatically. In order to do this, I had to go off spec and use GCC-specific code, which I find annoying. I guess it works. ü§∑---One of the reasons garbage-collected languages are the norm nowadays is because programmers supposedly can't manage memory themselves. Languages like Rust and Zig use ownership models to manage memory more deterministically. How hard can it be, though? For every &lt;code&gt;malloc&lt;/code&gt;, you must also &lt;code&gt;free&lt;/code&gt;. Or so I thought. This is one of those situations where I'd expect static analysis tools to shine. I'd love it if a linter could yell at me in my editor. I haven't found such a tool. I have tried using Valgrind, but I don't understand what it's telling me. It says I'm leaking memory, but I don't understand how. Each &lt;code&gt;malloc&lt;/code&gt; has a corresponding &lt;code&gt;free&lt;/code&gt;, and yet.---The biggest things I miss from every language I've ever touched are dynamic arrays and associative arrays. Despite the fact that every non-trivial program ever written uses them, the C standard library doesn't have either of these. Because I want to be a "real programmer" I tried to implement them myself. üôÑ I guess I'm just not hardcore enough. I didn't get a traditional Computer Science education, so I missed out on the joys of accomplishing these feats with the added pressure of a university environment. I understand both of these data structures &lt;em&gt;in principle&lt;/em&gt;. Doing it in practice is a different story. Especially trying to do it in a generic way that would work with any kind of data type.I could grab Glib or something that has these structures built-in, but then I lose portability and I have to accept the opinionated way in which the authors have chosen to implement them. I hate opinionated libraries.---What's the problem? I'm not a "real programmer", I guess. Despite supporting my family for over a decade on a very generous income afforded to me by my profession, despite earning the respect of my coworkers, despite being a tutor and mentor to others, I'm not a "real programmer". I guess I'm just too soft. I grew up with garbage collectors and Visual Studio. I never had it rough. I was born just barely too late for the microcomputers that came with thick manuals and booted into BASIC interpreters. I'm obsessed with dumb concepts like "correctness" and "best practices". I am not a "real programmer". But I guess I want to be...?Linus Torvalds, an impressive programmer, but not exactly an ideal coworker, said "If it compiles, it is good; if it boots up, it is perfect." I wish I could be that cavalier.&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/am-i-a-real-developer-yet/</guid>
      <pubDate>Sat, 08 Jul 2023 18:41:02 +0000</pubDate>
    </item>
    <item>
      <title>Can I Roll My Own State Container For React?</title>
      <link>https://jaysherby.com/can-i-roll-my-own-state-container-for-react/</link>
      <description>&lt;p&gt;I was recently chatting with some former coworkers. One of them asked if Redux was still the go-to library for React app state management.I was surprised when others brought up some libraries I hadn't heard of called &lt;a href="https://recoiljs.org/"&gt;Recoil&lt;/a&gt; and &lt;a href="https://github.com/pmndrs/zustand"&gt;Zustand&lt;/a&gt;. I've reviewed them briefly, and it looks like they each bring interesting benefits and improvements over bog-standard Redux. Without having actually used either of them, that's really the most I can say about them.The last time I worked on a React app that benefited from the type of state container that Redux provides was around five years ago. I used Redux at that time. I've still used React since then, but the apps I've worked on have synchronized their Big State‚Ñ¢Ô∏è via their back-ends. I've found React's built-in hooks to be adequate for the rest. I'll reach for third-party libraries for form management, but otherwise the built-in hooks and custom hooks composed from them tend to do the job.At some point since the &lt;code&gt;useReducer&lt;/code&gt; hook landed, the notion that Redux isn't really necessary anymore stuck in my brain. You should be able to roll your own version using React's core features now, right?When I think of Redux, I think of three main features: The Reducer‚Ñ¢Ô∏è, the ability to access the state container from arbitrary components without having to pass stuff up and down the tree, and selectors. In the version I'm going to throw together here, the reducer and state container are provided by &lt;code&gt;useReducer&lt;/code&gt;, and the arbitrary component state container access will be provided by a React context. Selectors should be an implementation detail. üëÄLet's start with the bit that should be the same across any application.&lt;code&gt;tsximport React, { Context, Dispatch, ReactNode, Reducer, createContext, useContext, useReducer} from "react";type StoreContext&amp;lt;S, A&amp;gt; = Context&amp;lt;{ state: S; dispatch: Dispatch&amp;lt;A&amp;gt; }&amp;gt;;interface Props&amp;lt;S, A&amp;gt; { context: StoreContext&amp;lt;S, A&amp;gt;; reducer: Reducer&amp;lt;S, A&amp;gt;; children: ReactNode;}export function createStoreContext&amp;lt;S, A&amp;gt;(initialState: S): StoreContext&amp;lt;S, A&amp;gt; { return createContext({ state: initialState, dispatch: (() =&amp;gt; {}) as Dispatch&amp;lt;A&amp;gt; });}export function Store&amp;lt;S, A&amp;gt;({ children, context: Context, reducer}: Props&amp;lt;S, A&amp;gt;) { const { state: initialState } = useContext(Context); const [state, dispatch] = useReducer(reducer, initialState); return ( &amp;lt;Context.Provider value={{ state, dispatch }}&amp;gt;{children}&amp;lt;/Context.Provider&amp;gt; );}&lt;/code&gt;I've created a convenience function called &lt;code&gt;createStoreContext&lt;/code&gt; that takes an initial state and returns a context that will hold your app's state. The only thing that might be considered tricky here is that I'm using a no-op function as the placeholder for the dispatch function until the context is provided to the &lt;code&gt;Store&lt;/code&gt; component.The &lt;code&gt;Store&lt;/code&gt; component is what I'd call a surrogate context provider component. I made up that word salad just now to describe it, but it's a common pattern. It acts like a context provider, but it adds the logic for how the context is going to be used. In this case, that logic is just the &lt;code&gt;useReducer&lt;/code&gt; function.Next, I'll set up the app-specific types and logic for a contrived example app. This app has two pieces of state: a string and a number.&lt;code&gt;typescriptimport { createStoreContext } from "./Store";interface State { number: number; text: string;}type Action = | { type: "INCREMENT_NUMBER" | "DECREMENT_NUMBER" } | { type: "CHANGE_TEXT"; updatedText: string };const initialState: State = { number: 50, text: "initial text"};function reduceText(state: string, action: Action) { switch (action.type) { case "CHANGE_TEXT": return action.updatedText; default: return state; }}function reduceNumber(state: number, action: Action) { switch (action.type) { case "INCREMENT_NUMBER": return state + 1; case "DECREMENT_NUMBER": return state - 1; default: return state; }}export function reducer(state: State, action: Action): State { return { number: reduceNumber(state.number, action), text: reduceText(state.text, action) };}export const AppState = createStoreContext&amp;lt;State, Action&amp;gt;(initialState);&lt;/code&gt;I hope you were able to stay awake through all that. I made some opinionated choices about composing reducers, but any valid reducer function will work.I decided to create the context instance in this file rather than in the root &lt;code&gt;App&lt;/code&gt; component to make the dependency tree a little cleaner since every component that connects to our state container will depend on that context instance.Speaking of the root &lt;code&gt;App&lt;/code&gt; component, I'll make that now.&lt;code&gt;tsximport React from "react";import { Store } from "./Store";import { AppState, reducer } from "./state";import TextView from "./TextView";import NumberView from "./NumberView";import "./styles.css";export default function App() { return ( &amp;lt;Store context={AppState} reducer={reducer}&amp;gt; &amp;lt;div className="App"&amp;gt; &amp;lt;h1&amp;gt;Here's a sample app!&amp;lt;/h1&amp;gt; &amp;lt;TextView /&amp;gt; &amp;lt;NumberView /&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/Store&amp;gt; );}&lt;/code&gt;I'm passing the &lt;code&gt;AppState&lt;/code&gt; context into the &lt;code&gt;Store&lt;/code&gt; component as well as the app's reducer. I'm going to write two components that directly utilize the app state: TextView and NumberView.&lt;code&gt;tsximport React, { ChangeEventHandler, useCallback, useContext } from "react";import { AppState } from "./state";export default function TextView() { const { state, dispatch } = useContext(AppState); const handleChange = useCallback&amp;lt;ChangeEventHandler&amp;lt;HTMLInputElement&amp;gt;&amp;gt;( (event) =&amp;gt; dispatch({ type: "CHANGE_TEXT", updatedText: event.target.value }), [dispatch] ); return ( &amp;lt;div&amp;gt; &amp;lt;h2&amp;gt;Change my text!&amp;lt;/h2&amp;gt; &amp;lt;input type="text" value={state.text} onChange={handleChange} /&amp;gt; &amp;lt;/div&amp;gt; );}``````tsximport React, { ReactEventHandler, useCallback, useContext } from "react";import { AppState } from "./state";export default function NumberView() { const { state, dispatch } = useContext(AppState); const handleIncrement = useCallback&amp;lt;ReactEventHandler&amp;gt;( (event) =&amp;gt; dispatch({ type: "INCREMENT_NUMBER" }), [dispatch] ); const handleDecrement = useCallback&amp;lt;ReactEventHandler&amp;gt;( (event) =&amp;gt; dispatch({ type: "DECREMENT_NUMBER" }), [dispatch] ); return ( &amp;lt;div&amp;gt; &amp;lt;h2&amp;gt;Current number: {state.number}&amp;lt;/h2&amp;gt; &amp;lt;button onClick={handleIncrement}&amp;gt;+&amp;lt;/button&amp;gt; &amp;lt;button onClick={handleDecrement}&amp;gt;-&amp;lt;/button&amp;gt; &amp;lt;/div&amp;gt; );}&lt;/code&gt;At this point, the app works. I'm using React's &lt;code&gt;useContext&lt;/code&gt; hook directly to access the state and the dispatch function.&lt;img alt="My beautiful app" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/jaysherby-1688085276-0.png"/&gt;But there's a pretty big problem. Because each component depends on the entire application state, every component re-renders on every state change regardless of whether the change was actually relevant to the component in question. In other words, I need selectors.This is where my hand-rolled scheme falls apart. There's currently no way to avoid these types of unnecessary re-renders while using a single context. No matter what kind of memoization strategies you employ, the &lt;code&gt;useContext&lt;/code&gt; hook will trigger a re-render of every component attached to the context upon a state change.I'm not the only one out there who thinks this is a glaring omission from React's API. There's &lt;a href="https://github.com/reactjs/rfcs/pull/119"&gt;an open RFC&lt;/a&gt; for just such a feature that's been hanging out since 2019.If you were &lt;em&gt;really&lt;/em&gt; determined to write your own app container only using React primitives, the next place I'd look would be the relatively new &lt;a href="https://react.dev/reference/react/useSyncExternalStore"&gt;&lt;code&gt;useSyncExternalStore&lt;/code&gt; hook&lt;/a&gt;. I'm almost certain you can achieve the desired effects with that hook, but it's just not as interesting. You'd be building Redux from bare JavaScript to interface with the &lt;code&gt;useSyncExternalStore&lt;/code&gt; API without being able to lean on any other built-in hooks. At that point, just use a library.## UPDATEAfter discussing this with a friend and former co-worker[^1], this could probably also be accomplished using a higher-order component to conditionally re-render components. I personally dislike the ergonomics of HOCs in this era of hooks. Besides &lt;code&gt;React.memo&lt;/code&gt;, which could be argued isn't a "true" HOC, I haven't had to use HOCs in years.---[^1]: Shout out to &lt;a href="https://medium.com/@wtgjxj"&gt;James&lt;/a&gt; this time.&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/can-i-roll-my-own-state-container-for-react/</guid>
      <pubDate>Thu, 29 Jun 2023 22:29:33 +0000</pubDate>
    </item>
    <item>
      <title>Typescript, Show Me The Whole Error!</title>
      <link>https://jaysherby.com/typescript-show-me-the-whole-error/</link>
      <description>&lt;p&gt;Has this ever happened to you? Typescript throws an error about two types being incompatible, but the differing parts of the types is truncated from the error!This is the default behavior for some reason. I'm not a fan. I know long error messages scare a lot of developers, but I'd rather have a long and helpful error message than a short and useless one.Good news! Error message salvation is one &lt;code&gt;tsconfig.json&lt;/code&gt; setting away. Set &lt;a href="https://www.typescriptlang.org/tsconfig#noErrorTruncation"&gt;&lt;code&gt;noErrorTruncation&lt;/code&gt;&lt;/a&gt; to &lt;code&gt;false&lt;/code&gt; and you'll be all set.Personally, I'd rather opt in to truncation than opt out, but at least it's an easy fix.&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/typescript-show-me-the-whole-error/</guid>
      <pubDate>Wed, 21 Jun 2023 19:26:47 +0000</pubDate>
    </item>
    <item>
      <title>React Hook Form: A Leaky Abstraction</title>
      <link>https://jaysherby.com/react-hook-form-a-leaky-abstraction/</link>
      <description>&lt;p&gt;I have used a handful of helper libraries for wrangling form state in React over the course of my career. In my current job, we're using React Hook Form[^1], specifically v6, although I think my story might still apply to those using the newer v7 branch. I enjoy working with it for the most part. I've found it to be a lighter-weight alternative to Formik.However, this past week I ran into a weird edge case. To set the stage, a friend and colleague[^2] was working on a ticket to add a field to an existing form that's powered by React Hook Form. The new field was a &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; style dropdown menu with dynamically populated options based on the user's configuration. This field is required. However, most users will probably only have one option. In that case, we chose not to show this new field at all. Instead, that field will automatically default to the only option available to that user.Here's a tiny example:&lt;code&gt;typescriptconst { groups } = useUser();const userHasMultipleGroups = groups.length &amp;gt; 1;const defaultValues = { name: '', group: userHasMultipleGroups ? '' : groups[0].value,};const { control, handleSubmit, formState: { errors } } = useForm({ defaultValues, resolver: yupResolver(schema),});// ...snip...&amp;lt;Controller as={&amp;lt;TextInput /&amp;gt;} control={control} name="name"/&amp;gt;{userHasMultipleGroups &amp;amp;&amp;amp; ( &amp;lt;Controller as={&amp;lt;SelectInput /&amp;gt;} control={control} name="group" options={groups} /&amp;gt;)}&lt;/code&gt;Let me break this down in words briefly to make sure we're on the same page.This component checks a user's groups, which is provided by a custom hook called &lt;code&gt;useUser&lt;/code&gt;.The form's default value for &lt;code&gt;group&lt;/code&gt; is a blank string if the given user has multiple groups available to them. If the user has just one group, which is the most common case, their group's value is automatically the default value for that form input.Later, we choose to only render the &lt;code&gt;group&lt;/code&gt; field if there are multiple choices to choose from.In practice, &lt;em&gt;this won't work&lt;/em&gt;. The form works as expected when the user belongs to multiple groups. When there's only one group available, this will fail. On submit, the validation will fail, claiming there's no value for the &lt;code&gt;"group"&lt;/code&gt; field.What gives? We set the default value for that field! Making things even more confusing, if we add a logging statement like &lt;code&gt;console.log(getValues())&lt;/code&gt;, it will log an empty object until we start interacting with the form. It's like the default values aren't being used at all.After futzing around for a couple of hours and getting nowhere, here's the result.1. &lt;code&gt;getValues()&lt;/code&gt; doesn't know anything about &lt;code&gt;defaultValues&lt;/code&gt;.2. The values in &lt;code&gt;defaultValues&lt;/code&gt; won't propagate into the form's values unless the form &lt;em&gt;actually renders an input&lt;/em&gt;.3. Similarly, using &lt;code&gt;setValue()&lt;/code&gt; in a &lt;code&gt;useEffect&lt;/code&gt; hook, for example, won't work for any field in the form that doesn't have an actual input rendered on the page.This is actually mentioned in the documentation, but it's not exactly front and center.&amp;gt; - Its not default state for the form, to include additional form values:&amp;gt;&amp;gt;     1. Register hidden input: &lt;code&gt;&amp;lt;input type="hidden" ref={register} name="test" /&amp;gt;&lt;/code&gt;&amp;gt;&amp;gt;    2. Combine values at onSubmit callback.&amp;gt;&amp;gt; -- &lt;a href="https://legacy.react-hook-form.com/v6/api#useForm"&gt;React Hook Form docs for &lt;code&gt;useForm&lt;/code&gt;, specifically under &lt;code&gt;defaultValues&lt;/code&gt;&lt;/a&gt;This little tidbit is what tipped me off to this situation.[^3]There's also a clue in the FAQs for v7 in the answer for the question, "How to work with modal or tab forms?"&amp;gt; It's important to understand React Hook Form embraces native form behavior by storing input state inside each input (except custom &lt;code&gt;register&lt;/code&gt; at &lt;code&gt;useEffect&lt;/code&gt;). One of the common misconceptions is that when working with modal or tab forms, by mounting and unmounting form/inputs that inputs state will remain. That is incorrect. Instead, the correct solution would be to build a new form for your form inside each modal or tab and capture your submission data in local or global state and then do something with the combined data.&amp;gt;&amp;gt; -- &lt;a href="https://react-hook-form.com/faqs#Howtoworkwithmodalortabforms"&gt;React Hook Form FAQs, "How to work with modal or tab forms?"&lt;/a&gt;These bits of information, pieced together, answer the question of why every piece of data in a React Hook Form-based form must have an input associated with it, even if it's a hidden input.## A Leaky AbstractionI don't use the term "leaky abstraction" lightly because it has become a generic insult thrown around by developers online, lodged against any tool or library they don't like. However, this is a textbook leaky abstraction in my mind.### The AbstractionI'm using React Hook Form to reduce the boilerplate of form state management.  I could write my own state management for any given form using only React built-ins like &lt;code&gt;useReducer&lt;/code&gt; or I guess a dozen or so &lt;code&gt;useState&lt;/code&gt; instances. But it's tedious, repetitive, and error-prone. If I choose to build this using React built-ins, the state of the form and the form inputs are completely separated from each other. If I should choose to conditionally render any of my input elements, the state representing the data of those inputs is unaffected. The data and its visual representation aren't dependent on one another.[^4]I would assume when I'm using a form state management library, it's doing this same stuff for me behind the scenes. It's just handling all of that tedious boilerplate code for me, but I assume the semantics, like a separation between data and view, would remain the same.### The LeakAs we saw, my assumptions were incorrect. As the docs said, React Hook Form depends intimately upon the input elements on the page to hold its state. This is an implementation detail of the library that you generally don't need to know or care about... until you do.I'm willing to believe that my form is the edge case here. Most forms on the web can be built using React Hook Form and you'll never need to know about these implementation details. Most forms probably show all inputs every render. But we were improving UX so that users don't need to care about a required dropdown input if there's only one option to select from.I want to make it clear here that I'm not calling React Hook Form a leaky abstraction as an insult. I still mostly like the library, even if it's not built the same way I would choose to build it. I'm going to continue using it. I still prefer it to Formik. But this is an implementation detail, or perhaps a consequential design philosophy, that users should be aware of.---[^1]: Because I regularly confuse the name as &lt;code&gt;react-form-hook&lt;/code&gt;, I've given up and lovingly refer to it as &lt;code&gt;react-man-door-hand-hook-car-door&lt;/code&gt; instead. I'm sure my coworkers have become tired of this joke.[^2]: This is the second shout out to Emily on my blog! ‚ù§Ô∏è[^3]: Note the docs' mention of combining the default data at the time of submission of the form. I did consider this approach after I'd already implemented an input component that intelligently used a hidden input when it made sense. But this approach wouldn't have worked in my case because I'm using schema validation to validate the form. Before I even got the chance to inject my default value, the validation would have failed and my submit function wouldn't have been called. I could have moved validation into my submit function, but then I'd lose the convenience of React Hook Form's built-in error messages. I guess I could make two versions of the schema, but who knows what unexpected gotchas that would uncover? It's a house of cards situation, and using a hidden input was the best choice.[^4]: It's been a couple of years since I've used Formik, but this is roughly how it operates IIRC. When I switched to working on a codebase using React Hook Form, I assumed this was still the case.&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/react-hook-form-a-leaky-abstraction/</guid>
      <pubDate>Sat, 10 Jun 2023 15:33:11 +0000</pubDate>
    </item>
    <item>
      <title>A formal letter of apology to Batman</title>
      <link>https://jaysherby.com/a-formal-letter-of-apology-to-batman/</link>
      <description>&lt;p&gt;Dear Batman,I hope this letter finds you well. It is with deep remorse and a heavy heart that I pen this formal apology to you. Recently, a rather unpleasant incident occurred within the confines of the sacred Bat Cave, an incident for which I must take full responsibility. I am truly sorry to inform you that I, Robin, was the culprit behind the unfortunate emission of flatulence in our esteemed headquarters.As a trusted member of the Dynamic Duo and your loyal crime-fighting companion, I understand the solemnity and sanctity of our crime-fighting operations. The Bat Cave serves as the epicenter of our collaborative efforts, where we strategize, analyze, and prepare for the challenges that await us in the perilous streets of Gotham City. It is a place of utmost professionalism, focus, and, of course, fresh air.The events leading up to the aforementioned mishap were rather innocuous. In hindsight, I admit to having consumed a hearty meal of beans, onions, and broccoli before our mission. My intentions were pure, as I sought to replenish my energy reserves for the arduous night of crime-fighting that lay ahead. Alas, the subsequent effects were unforeseen and regrettable.During our pursuit of the nefarious Joker, my attempt to stifle a brewing gaseous release proved futile. In a moment of weakness, a quiet but noxious emission escaped, infiltrating the air within the Bat Cave with an unsavory odor that cannot be easily forgotten. I apologize for subjecting you to such an unseemly and unbecoming spectacle, especially given the seriousness of our line of work.Batman, please understand that this occurrence was an unfortunate accident, an unintended consequence of my bodily functions. It was never my intention to compromise the solemnity and atmosphere of our hallowed sanctuary. I humbly request your forgiveness for this momentary lapse in decorum, as I hold the utmost respect for you, our partnership, and the significance of our mission.Moving forward, I assure you that I will exercise greater caution regarding my dietary choices and their potential consequences. I will make a conscious effort to avoid any food items that could contribute to a recurrence of such regrettable incidents. Your trust and faith in my abilities are of utmost importance to me, and I am committed to upholding the professionalism and dignity befitting the role of Robin.Once again, Batman, I extend my sincere apologies for defiling the sanctity of the Bat Cave with my untimely flatulence. I hope you can find it in your heart to forgive me, and that our partnership may continue with the same unwavering strength and camaraderie as before.With the sincerest apologies and the utmost respect,Robin&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/a-formal-letter-of-apology-to-batman/</guid>
      <pubDate>Thu, 18 May 2023 19:35:52 +0000</pubDate>
    </item>
    <item>
      <title>HATEOAS: I want to believe</title>
      <link>https://jaysherby.com/hateoas-i-want-to-believe/</link>
      <description>&lt;p&gt;There's a growing voice in the web dev community. I'm going to call them "web fundamentalists". They hold certain beliefs about web development.- Single-page apps (SPAs) make for a poor user experience.- SPAs are bad for accessibility.- Good APIs make semantically correct use of more of the HTTP verbs than just GET and POST.- It's okay for an application server to return HTML.- The web should be generally usable with JavaScript disabled.- Cookie-based sessions are better than JSON Web Tokens.- JavaScript doesn't need a compiler.- A web API that accepts and returns JSON isn't necessarily REST.- Web applications should be offline-first.- Accessibility encompasses more than assistive technologies. An accessible application is tolerant of old hardware, old software, and poor connectivity.- Boring tech makes for reliable tech.- Progressive enhancement is better than graceful degradation, but both are better than nothing at all.I agree with most, if not all, of these beliefs, in principle. However, I find that many of these beliefs are often overly idealistic and can fall apart when theory meets practice.## Uphill both waysJust like many forms of fundamentalism, this form is probably influenced by how quickly and drastically change has occurred. Web development looks completely different today than it did 10 years ago. In a rapidly changing environment, it's easy to fall prey to rose-tinted nostalgia.I remember when a web application generated all of a page's HTML on the server side. Slowly but surely, more and more logic would find its way into the templates. This was bad because template logic was generally slower than non-template logic. I have worked on applications where more than half of a server request's time was spent rendering the HTML. It was a bad time. In the short term, people started pushing so-called "logic-less templates". &lt;a href="https://en.wikipedia.org/wiki/Mustache_%28template_system%29"&gt;Mustache templates&lt;/a&gt; became the new hotness.Longer term, the prevailing wisdom was that we needed to stop treating web browsers like thin clients. Can you believe there was ever any doubt? Web browsers are the thickest of thick clients. The web browser is the new operating system. Look at Chromebooks! People gladly buy laptops that can only run a glorified web browser.If more than half of your server's workload is building HTML, basically shuffling strings around, you're paying to do something your client's devices are capable of doing on your behalf. Plus, there's so much redundant data going over the wire. You're sending your web app's navigation code for every page request. Ideally, you should amortize the templating and rendering by making your users' devices handle it. Your server doesn't have to do it, so you don't have to pay for the computer time or the data transfer.I was there at the time and this made perfect sense. Your server only has to deal in pure data and your front-end code only needs to worry about displaying that data and performing server requests. This is what led to REST APIs and SPAs.But modern solutions call for modern problems and now we have to care about the efficiency with which data is represented and queried. None of that mattered previously because it all happened on the server. Your browser requested one whole page and the server returned one whole page. Since accessing the data and applying it to a template was essentially a single task accomplished by a single program running on a single machine, tightly coupling them wasn't a big deal.With SPAs, you're encouraged to think of your server and your front-end as two different programs that happen to talk to one another. Wouldn't it be nice if your data representation wasn't tied to any particular display format? Your data is its own platonic ideal of entities and relationships, not all that different from a database. In fact, your application server isn't much more than a thin wrapper around your database. The only extra stuff it needs to do is authentication, authorization, data serialization (into JSON, the web's data serialization format of choice), and some data validation that's more application-specific than you can probably represent in a database alone.## R-E-S-T, find out what it means to meRepresentational state transfer, or REST, was the solution for what protocol the web was going to standardize on for this new world of front-ends and back-ends. Almost.REST, at its heart, is basically a remote procedure call interface that plays to the strengths of the web's existing technology. Specifically, URIs, HTTP verbs, HTTP response codes, and browser caches.There's a problem, though, at least if you ask a REST purist. Nobody uses it "correctly".[^1] [^2] You're technically not REST unless you're using HATEOAS.## Only love can conquer HATEOASHypermedia as the engine of application state, or HATEOAS. The name that just rolls off the tongue.I'm not going to explain it in extreme detail here, but the idea, roughly, is that your API returns JSON objects that look like this:&lt;code&gt;json{ "author": { "id": 123, "firstName": "Stephen", "lastName": "King", "birthday": "1947-09-21T05:00:00.000Z", "links": { "books": "/authors/123/books" } }}&lt;/code&gt;A given model contains all the information you'd expect, plus a &lt;code&gt;links&lt;/code&gt; (or &lt;code&gt;_links&lt;/code&gt; in some implementations) object containing the URIs of various related models.Honestly, at first blush, this seems like a really reasonable way to do things. It could benefit from some kind of library or something to automatically parse these things, but it's simple enough that almost any developer could probably roll their own.However, there's at least one glaring flaw to me where the rubber meets the road, and it's when you need to traverse more than one relationship in the graph.Suppose, for example, my front-end is consuming an API like the one that would generate that Stephen King model above. I want to get a list of publishers who have published Stephen King's books. Imagine the &lt;code&gt;book&lt;/code&gt; model looks like this:&lt;code&gt;json{ "book": { "id": 720, "title": "The Dead Zone", "pages": 428, "publicationDate": "1979-08-30T05:00:00.000Z", "isbn": "978-0-670-26077-5", "links": { "author": "/authors/123", "publisher": "/publishers/902" } }}&lt;/code&gt;What I can do is request all of King's books with &lt;code&gt;GET /authors/123/books&lt;/code&gt; and then call a &lt;code&gt;GET&lt;/code&gt; on the &lt;code&gt;publisher&lt;/code&gt; link for each &lt;code&gt;book&lt;/code&gt;.However, subjectively speaking, this sucks. I don't know if you've ever had to work with an app that works this way, but it's a bad time for users. Best case scenario, this is going to take a lot of time and a lot of bandwidth due to the overhead of making one API per book. Even with HTTP/3 or whatever, where HTTP is no longer one transaction per connection. And the browser is going to choke if you fire off dozens of AJAX[^3] requests at once.[^4]Okay, okay. I know that what I described is a ridiculous approach that I definitely have never seen in the wild, much less worked on an app that does it. What we'd actually do is add a new link to the &lt;code&gt;author&lt;/code&gt; model that does the relational hop for us on the back-end and return the results we want. Something like &lt;code&gt;/author/123/publishers&lt;/code&gt;.I don't know about you, but I hate adding boilerplate endpoints to apps. Seems like something metaprogramming would handle for you. There would need to be some guidelines from the programmer, though. If I just gave the program my database and told it to figure it out, I'd end up with technically valid but meaningless relationships.[^5] Not to mention the cycles.What I find myself describing almost exists already, but it's not HATEOAS. It's GraphQL.I know there are a lot of GraphQL haters out there. And I totally understand. It's not as simple to add GraphQL to an app as it is to incrementally add endpoints that speak JSON. Most GraphQL implementations, both front-end and back-end, are heavy-weight dependencies. And GraphQL itself is not a panacea for the problem of connecting front-end and back-end.I've used GraphQL a few times in my career. Never for anything that's Serious Business‚Ñ¢Ô∏è. One time was in a Rails app. Another time was with Gatsby.What impressed me most about using it was the quality of the developer experience. Once you set up your models, querying data was a breeze. Introspection, auto-completion, and type inference made writing queries a joy. Having your data query live in the same file as your display logic was the same kind of revelation as when React gave us JSX and our templates could live with the display logic. It's a code locality win.Combine that with Prisma's tooling[^6] on the back-end and the need to write boilerplate endpoints virtually disappears.GraphQL also solves the problem of data validation and sanitation, which HATEOAS doesn't address. Neither solves the problem of authorization, though. My point is that metaprogramming can't account for all the drudgery associated with a CRUD back-end.## The &lt;em&gt;other&lt;/em&gt; HATEOASLet's get pedantic for a second. I talked about HATEOAS and I gave an example using JSON. The example I gave is an example of HATEOAS, but it's just one implementation. The most important word in HATEOAS is "hypermedia". The JSON examples I gave qualify as hypermedia because they have links.You know what else qualifies as hypermedia? The OG, HTML! Hyper text markup language.There are some folks who have made various tools exploring this idea of HTML as HATEOAS. Their latest creation is &lt;a href="https://htmx.org/"&gt;HTMX&lt;/a&gt;.I think it would be worth trying to create a web application using HTMX. The problem is, it bucks the trends of Big JavaScript‚Ñ¢Ô∏è.You can have the things that make development easier:- Templates and display logic live together- Data queries and display logic live together- No more endpoint boilerplate hell... maybe?The catch is, you'll be going back to rendering your HTML on the server side. But at least you don't have to render &lt;em&gt;entire&lt;/em&gt; pages per request. You can absolutely return page fragments that get inserted into an existing page.[^7]## Wrapping upI don't mean to be overly critical of the web dev hippies out there. I see you. I love you. I agree with you most of the time.I am critical of JSON HATEOAS implementations. Without some kind of tools for metaprogramming, I've found it leads to boilerplate fatigue. I don't enjoy churning out CRUD endpoints. I also hate when adding a new endpoint leads to a large impact radius in the code. Adding one new model, for example, is one thing. But then I have to modify all the endpoints of all the other models that contain relationships to the new endpoint that I want to utilize.I don't think JSON HATEOAS implementations are inherently bad. How can they be? They're implementations. I don't think hypermedia is bad. How could I? The whole web is built on it. I do think that hypermedia is often an inappropriate medium to drive some of the applications we use it for. But, to its credit, hypermedia is adaptable enough that we can usually bend it into something resembling the shape we desire.I have found that GraphQL, as a query language, seems a lot more powerful than JSON HATEOAS implementations to quickly produce maintainable web applications. But it's mostly due to the quality of its assistive tools. If I had to write GraphQL queries by hand with nothing to help me, I would probably be complaining about it, too.---[^1]: REST is an architectural style, not a standard. Therefore, "correctness" is up to interpretation.[^2]: "Correct" REST can be pretty controversial. For example, server-side sessions are technically disallowed, whether that means using a session ID in a cookie or as part of the URI. This is because sessions introduce state, whereas REST is supposed to be stateless. Also, sessions mean there is data which is opaque to the user, which is deemed a security and privacy risk. But what's the alternative? Sending credentials along with every request that needs it seems less secure to me (assuming sessions are implemented securely, which is a &lt;em&gt;big&lt;/em&gt; assumption, I know). JSON Web Tokens are not opaque, for better or worse, but it's considered better practice to prefer a session token for systems where the application server &lt;em&gt;is&lt;/em&gt; the authentication authority.[^3]: It's 2023. Are we still calling this AJAX? This is just how the web works now, right? ü§∑[^4]: Ask me how I know.[^5]: My library card catalog example is too simple for an example, but anyone who has worked with a production database knows what I'm talking about.[^6]: I'm not a fan of ORMs generally, but I've found Ecto and Prisma to be the least objectionable in my experience so far. Neither is perfect, but I'm particularly impressed by Prisma's tooling, which generates Typescript types for your database models. We need more metaprogramming tools like this.[^7]: I haven't tried this myself yet, but I'm very intrigued by the concept. The ultimate would be if all response bodies are complete HTML pages. I'm not sure how that would work. Iframes everywhere seems too heavy. It just feels weird to me to return HTML &lt;em&gt;fragments&lt;/em&gt; from a back-end.&lt;/p&gt;
</description>
      <author>hidden (jaysherby)</author>
      <guid isPermaLink="false">https://jaysherby.com/hateoas-i-want-to-believe/</guid>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
